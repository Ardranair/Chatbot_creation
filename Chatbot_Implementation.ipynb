{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit langchain-openai langchain-google-genai langchain-groq langchain-core faiss-cpu pypdf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hUNh7YAhKG_",
        "outputId": "c3357c1a-4f4a-4528-aecd-1033ccc02a3e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.12/dist-packages (1.51.0)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (1.0.3)\n",
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.12/dist-packages (3.1.0)\n",
            "Requirement already satisfied: langchain-groq in /usr/local/lib/python3.12/dist-packages (1.0.1)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.4.0)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow<22,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.109.1)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage<1.0.0,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (2.11.10)\n",
            "Requirement already satisfied: groq<1.0.0,>=0.30.0 in /usr/local/lib/python3.12/dist-packages (from langchain-groq) (0.36.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.4.43)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (6.0.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.12.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (2.28.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (1.76.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (0.28.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.11.12)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (1.72.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (1.71.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq<1.0.0,>=0.30.0->langchain-groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1.0.0,>=0.30.0->langchain-groq) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.29.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/project\")\n"
      ],
      "metadata": {
        "id": "JKqRLg-G36H_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key\"\n",
        "os.environ[\"GROQ_API_KEY\"] = \"your_groq_api_key\"\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"your_google_api_key\"\n"
      ],
      "metadata": {
        "id": "7ROmZaNe_AFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.llm import load_llm\n",
        "\n",
        "print(\"Testing OpenAI model:\")\n",
        "model = load_llm(\"openai\")\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQ6tKxZe5ECj",
        "outputId": "10922c07-6989-478b-b066-1e4f93cf066a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing OpenAI model:\n",
            "async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7e6db7e16870> root_async_client=<openai.AsyncOpenAI object at 0x7e6db7e17650> model_name='gpt-4o-mini' model_kwargs={} stream_usage=True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from config.config import OPENAI_API_KEY, GROQ_API_KEY, GOOGLE_API_KEY\n",
        "\n",
        "def load_model(provider=\"openai\"):\n",
        "    if provider == \"openai\":\n",
        "        return ChatOpenAI(api_key=OPENAI_API_KEY, model=\"gpt-4o-mini\")\n",
        "    elif provider == \"groq\":\n",
        "        return ChatGroq(api_key=GROQ_API_KEY, model=\"llama3-8b-8192\")\n",
        "    elif provider == \"gemini\":\n",
        "        return ChatGoogleGenerativeAI(google_api_key=GOOGLE_API_KEY, model=\"gemini-pro\")\n",
        "    else:\n",
        "        raise ValueError(\"Invalid LLM provider\")\n"
      ],
      "metadata": {
        "id": "Pf7eSbgx5JpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/project/models/llm.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cduEfDLC4pdQ",
        "outputId": "2e077323-9223-4368-98e1-25679abcbc26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "from langchain_openai import ChatOpenAI\n",
            "from langchain_groq import ChatGroq\n",
            "from langchain_google_genai import ChatGoogleGenerativeAI\n",
            "from config.config import OPENAI_API_KEY, GROQ_API_KEY, GOOGLE_API_KEY\n",
            "\n",
            "def load_llm(provider=\"openai\"):\n",
            "    if provider == \"openai\":\n",
            "        return ChatOpenAI(api_key=OPENAI_API_KEY, model=\"gpt-4o-mini\")\n",
            "    elif provider == \"groq\":\n",
            "        return ChatGroq(api_key=GROQ_API_KEY, model=\"llama3-8b-8192\")\n",
            "    elif provider == \"gemini\":\n",
            "        return ChatGoogleGenerativeAI(google_api_key=GOOGLE_API_KEY, model=\"gemini-pro\")\n",
            "    else:\n",
            "        raise ValueError(\"Invalid LLM provider\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from config.config import OPENAI_API_KEY, GROQ_API_KEY, GOOGLE_API_KEY\n",
        "\n",
        "def load_model(provider=\"openai\"):\n",
        "    \"\"\"Loads selected Large Language Model.\"\"\"\n",
        "\n",
        "    if provider == \"openai\":\n",
        "        return ChatOpenAI(\n",
        "            api_key=OPENAI_API_KEY,\n",
        "            model=\"gpt-4o-mini\"\n",
        "        )\n",
        "\n",
        "    elif provider == \"groq\":\n",
        "        return ChatGroq(\n",
        "            api_key=GROQ_API_KEY,\n",
        "            model=\"llama3-8b-8192\"\n",
        "        )\n",
        "\n",
        "    elif provider == \"gemini\":\n",
        "        return ChatGoogleGenerativeAI(\n",
        "            google_api_key=GOOGLE_API_KEY,\n",
        "            model=\"gemini-pro\"\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"‚ùå Invalid provider. Use: 'openai', 'groq', or 'gemini'.\")\n"
      ],
      "metadata": {
        "id": "qF_TTxgcBbgr"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/project/models/llm.py\n"
      ],
      "metadata": {
        "id": "NMVOle1HB3ju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bcaefe4-5608-4adc-c621-89371beddb37"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "from langchain_openai import ChatOpenAI\n",
            "from langchain_groq import ChatGroq\n",
            "from langchain_google_genai import ChatGoogleGenerativeAI\n",
            "from config.config import OPENAI_API_KEY, GROQ_API_KEY, GOOGLE_API_KEY\n",
            "\n",
            "def load_llm(provider=\"openai\"):\n",
            "    if provider == \"openai\":\n",
            "        return ChatOpenAI(api_key=OPENAI_API_KEY, model=\"gpt-4o-mini\")\n",
            "    elif provider == \"groq\":\n",
            "        return ChatGroq(api_key=GROQ_API_KEY, model=\"llama3-8b-8192\")\n",
            "    elif provider == \"gemini\":\n",
            "        return ChatGoogleGenerativeAI(google_api_key=GOOGLE_API_KEY, model=\"gemini-pro\")\n",
            "    else:\n",
            "        raise ValueError(\"Invalid LLM provider\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from models.llm import load_llm\n",
        "\n"
      ],
      "metadata": {
        "id": "m6kqHQtmA5YZ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os\n",
        "\n",
        "sys.path.append(\"/content/project\")\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your_openai_key\"\n",
        "os.environ[\"GROQ_API_KEY\"] = \"your_groq_key\"\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"your_google_key\"\n",
        "\n",
        "from models.llm import load_llm\n",
        "\n",
        "print(\"Testing Model:\")\n",
        "model = load_llm(\"openai\")  # or \"groq\" if no OpenAI key\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfPB99nuNvFD",
        "outputId": "164a9083-27fb-43b8-f070-0edcabd6afec"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Model:\n",
            "async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7e6dba7fc830> root_async_client=<openai.AsyncOpenAI object at 0x7e6dba7fe240> model_name='gpt-4o-mini' model_kwargs={} stream_usage=True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from config.config import OPENAI_API_KEY, GROQ_API_KEY, GOOGLE_API_KEY\n",
        "\n",
        "def load_model(provider=\"openai\"):\n",
        "    \"\"\"Loads selected Large Language Model.\"\"\"\n",
        "\n",
        "    if provider == \"openai\":\n",
        "        return ChatOpenAI(\n",
        "            api_key=OPENAI_API_KEY,\n",
        "            model=\"gpt-4o-mini\"\n",
        "        )\n",
        "\n",
        "    elif provider == \"groq\":\n",
        "        return ChatGroq(\n",
        "            api_key=GROQ_API_KEY,\n",
        "            model=\"llama3-8b-8192\"\n",
        "        )\n",
        "\n",
        "    elif provider == \"gemini\":\n",
        "        return ChatGoogleGenerativeAI(\n",
        "            google_api_key=GOOGLE_API_KEY,\n",
        "            model=\"gemini-pro\"\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"‚ùå Invalid provider. Use: 'openai', 'groq', or 'gemini'.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "3MkqoSkO4Ll1"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key\"\n",
        "os.environ[\"GROQ_API_KEY\"] = \"your_groq_api_key\"\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"your_google_api_key\"\n"
      ],
      "metadata": {
        "id": "TdkgqU_vhVvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7904f65",
        "outputId": "49980a35-5ba7-4f0e-cb62-81952709839b"
      },
      "source": [
        "print(\"Testing OpenAI model:\")\n",
        "openai_model = load_model(\"openai\")\n",
        "print(f\"OpenAI Model: {openai_model}\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing OpenAI model:\n",
            "OpenAI Model: async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7e6dba515040> root_async_client=<openai.AsyncOpenAI object at 0x7e6dba514680> model_name='gpt-4o-mini' model_kwargs={} stream_usage=True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "google_model = load_model(\"gemini\")\n",
        "print(google_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVsBLBW_apQu",
        "outputId": "5b98c54d-7fbf-4c37-8d73-0112da80fe17"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model='models/gemini-pro' client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7e6db800f8c0> default_metadata=() model_kwargs={}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a46c9ccb",
        "outputId": "6274b3ba-b724-4ca5-a6d2-c16df2969aa8"
      },
      "source": [
        "print(\"\\nTesting Google model:\")\n",
        "google_model = load_model(\"gemini\")\n",
        "print(f\"Google Model: {google_model}\")"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing Google model:\n",
            "Google Model: model='models/gemini-pro' client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7e6dba5162a0> default_metadata=() model_kwargs={}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "def load_model(provider=\"openai\"):\n",
        "    if provider == \"openai\":\n",
        "        return ChatOpenAI(api_key=OPENAI_API_KEY, model=\"gpt-4o-mini\")\n",
        "\n",
        "    elif provider == \"groq\":\n",
        "        return ChatGroq(model=\"llama-3.1-70b-versatile\", api_key=GROQ_API_KEY)\n",
        "\n",
        "    elif provider == \"google\":\n",
        "        return ChatGoogleGenerativeAI(model=\"gemini-pro\", api_key=GOOGLE_API_KEY)\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "id": "txacVFpnhXJK"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEAFrg6-3bsM"
      },
      "outputs": [],
      "source": [
        "!mkdir project\n",
        "!mkdir project/config project/models project/utils project/data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile project/requirements.txt\n",
        "streamlit\n",
        "langchain\n",
        "langchain-community\n",
        "chromadb\n",
        "pypdf\n",
        "openai\n",
        "tiktoken\n",
        "python-dotenv\n",
        "langchain-openai\n",
        "langchain-groq\n",
        "langchain-google-genai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KU3KjeuUSdrk",
        "outputId": "5e79b73b-f8b1-4e39-94ac-05fefda6ffa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing project/requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile project/config/config.py\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
        "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAdOaHAJSnM6",
        "outputId": "66c1a164-6c51-4666-e271-33c00529f3cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing project/config/config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile project/models/llm.py\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from config.config import OPENAI_API_KEY, GROQ_API_KEY, GOOGLE_API_KEY\n",
        "\n",
        "def load_llm(provider=\"openai\"):\n",
        "    if provider == \"openai\":\n",
        "        return ChatOpenAI(api_key=OPENAI_API_KEY, model=\"gpt-4o-mini\")\n",
        "    elif provider == \"groq\":\n",
        "        return ChatGroq(api_key=GROQ_API_KEY, model=\"llama3-8b-8192\")\n",
        "    elif provider == \"gemini\":\n",
        "        return ChatGoogleGenerativeAI(google_api_key=GOOGLE_API_KEY, model=\"gemini-pro\")\n",
        "    else:\n",
        "        raise ValueError(\"Invalid LLM provider\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0orqaAI7Sxkb",
        "outputId": "d9887932-1c71-4c58-a209-a8e966ca8d37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing project/models/llm.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile project/models/embeddings.py\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from config.config import OPENAI_API_KEY\n",
        "\n",
        "def load_embeddings():\n",
        "    return OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6dU6hHiS8KJ",
        "outputId": "18983440-d99b-4a89-a217-9c1f03cae69c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing project/models/embeddings.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile project/utils/rag_utils.py\n",
        "import os\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.vectorstores import Chroma\n",
        "from models.embeddings import load_embeddings\n",
        "\n",
        "def load_documents(directory=\"project/data/\"):\n",
        "    docs = []\n",
        "    for file in os.listdir(directory):\n",
        "        if file.endswith(\".pdf\"):\n",
        "            loader = PyPDFLoader(os.path.join(directory, file))\n",
        "            docs.extend(loader.load())\n",
        "    return docs\n",
        "\n",
        "def build_vectorstore():\n",
        "    documents = load_documents()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "    embeddings = load_embeddings()\n",
        "    vectorstore = Chroma.from_documents(chunks, embeddings, persist_directory=\"db\")\n",
        "    return vectorstore\n",
        "\n",
        "def retrieve(query):\n",
        "    vectorstore = Chroma(persist_directory=\"db\", embedding_function=load_embeddings())\n",
        "    return vectorstore.similarity_search(query, k=3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWV1PnMXS_lY",
        "outputId": "664d8a8c-08cd-4ed0-a5f7-5a5019bc57dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing project/utils/rag_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile project/app.py\n",
        "import streamlit as st\n",
        "from models.llm import load_llm\n",
        "from utils.rag_utils import build_vectorstore, retrieve\n",
        "\n",
        "st.title(\"ü§ñ AI RAG Chatbot\")\n",
        "\n",
        "llm_choice = st.sidebar.selectbox(\"LLM Provider\", [\"openai\", \"groq\", \"gemini\"])\n",
        "response_mode = st.sidebar.radio(\"Response Style\", [\"Concise\", \"Detailed\"])\n",
        "build_clicked = st.sidebar.button(\"üìå Build Knowledge Base\")\n",
        "\n",
        "if build_clicked:\n",
        "    with st.spinner(\"Building knowledge base...\"):\n",
        "        build_vectorstore()\n",
        "    st.success(\"Knowledge base ready!\")\n",
        "\n",
        "query = st.text_input(\"Ask something...\")\n",
        "\n",
        "if st.button(\"Send\") and query:\n",
        "    llm = load_llm(llm_choice)\n",
        "    results = retrieve(query)\n",
        "\n",
        "    context = \"\\n\".join([doc.page_content for doc in results])\n",
        "    prompt = f\"\"\"\n",
        "    Use the following context to answer.\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question: {query}\n",
        "\n",
        "    Answer style: {response_mode}\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "    st.write(\"### Response:\")\n",
        "    st.write(response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAozntBkTCgR",
        "outputId": "b75834e9-7f35-4d9b-e3e5-8cd5751e0d35"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting project/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r chatbot_project.zip /content/project\n"
      ],
      "metadata": {
        "id": "pNVdGemtTKLK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ca9ee86-419e-4b14-e387-b3f59a15b6ef"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/project/ (stored 0%)\n",
            "  adding: content/project/utils/ (stored 0%)\n",
            "  adding: content/project/utils/rag_utils.py (deflated 58%)\n",
            "  adding: content/project/requirements.txt (deflated 39%)\n",
            "  adding: content/project/data/ (stored 0%)\n",
            "  adding: content/project/data/DATASET Collection.pdf (deflated 20%)\n",
            "  adding: content/project/config/ (stored 0%)\n",
            "  adding: content/project/config/__pycache__/ (stored 0%)\n",
            "  adding: content/project/config/__pycache__/config.cpython-312.pyc (deflated 37%)\n",
            "  adding: content/project/config/config.py (deflated 51%)\n",
            "  adding: content/project/app.py (deflated 45%)\n",
            "  adding: content/project/models/ (stored 0%)\n",
            "  adding: content/project/models/llm.py (deflated 55%)\n",
            "  adding: content/project/models/__pycache__/ (stored 0%)\n",
            "  adding: content/project/models/__pycache__/llm.cpython-312.pyc (deflated 32%)\n",
            "  adding: content/project/models/embeddings.py (deflated 29%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/project\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-b0-IgdHs8sU",
        "outputId": "e648ec62-f885-4027-b123-522fe2719195"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "app.py\tconfig\tdata  models  requirements.txt\tutils\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r chatbot_project.zip /content/project\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JSugnKetAhY",
        "outputId": "4989e6dc-6cc0-4cfa-d56d-1269cc645a86"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: content/project/ (stored 0%)\n",
            "updating: content/project/utils/ (stored 0%)\n",
            "updating: content/project/utils/rag_utils.py (deflated 58%)\n",
            "updating: content/project/requirements.txt (deflated 35%)\n",
            "updating: content/project/data/ (stored 0%)\n",
            "updating: content/project/data/DATASET Collection.pdf (deflated 20%)\n",
            "updating: content/project/config/ (stored 0%)\n",
            "updating: content/project/config/__pycache__/ (stored 0%)\n",
            "updating: content/project/config/__pycache__/config.cpython-312.pyc (deflated 37%)\n",
            "updating: content/project/config/config.py (deflated 54%)\n",
            "updating: content/project/app.py (deflated 45%)\n",
            "updating: content/project/models/ (stored 0%)\n",
            "updating: content/project/models/llm.py (deflated 55%)\n",
            "updating: content/project/models/__pycache__/ (stored 0%)\n",
            "updating: content/project/models/__pycache__/llm.cpython-312.pyc (deflated 32%)\n",
            "updating: content/project/models/embeddings.py (deflated 29%)\n"
          ]
        }
      ]
    }
  ]
}